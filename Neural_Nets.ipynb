{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets Models\n",
    "I ran this notebook in Google Colab, but some of the models can be loaded from saved weights in the ./models folder. I don't have all of them though because I had implemented checkpoints in the model callbacks and didn't realized that the filename passed was saving the weights in the local runtime instance rather than my drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbHoZyFZYlhN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import re\n",
    "import pickle as pkl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Neural Net Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding\n",
    "# Neural Net Training\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kjHQmfffYliD",
    "outputId": "01b7eed5-b7e7-4aeb-db60-1cb4a593236b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vwRnpK2rbBZ0",
    "outputId": "e9ce5632-1858-4696-94cf-04d6b164ce46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pb7xz10iYliV"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/datasets/text_with_placeholders.pkl', 'rb') as f:\n",
    "    text = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "766pjs04Ylic"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/datasets/keep_upper_case.pkl', 'rb') as f:\n",
    "    keep_upper_case = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4VB1p4YYlin"
   },
   "outputs": [],
   "source": [
    "text_pars = []\n",
    "for sublist in text:\n",
    "    text_pars.append([*['-BGP-']*10, *word_tokenize(sublist), *['-ENP-']*10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I wasn't running this in Colab, the cell would have been in a module. However, I didn't want to deal with the hassle of creating a module and uploading it into Drive and mounting it correctly and all that just for this one cell. (Note: it's not a function because it only needs to be run once per kernel).  \n",
    "This cell is doing two things. One, it's decapitalizing anything that's not in the ```keep_upper_case ``` set, which consists of terms like ```Mr.``` and ```Park``` that I want to be left as capitals. It will also not de-capitalize a single letter followed by a period (unles that letter is I), as there are several places when Austen refers to characters by initials, and I didn't import these to my proper nouns because they were too difficult to sub out.  \n",
    "The second thing that this cell does is fix tokenizing errors from the ```word_tokenize``` function. Several examples of such errors: Austens uses ```&c.``` as etc. ```word_tokenize``` splits this up into 2-3 tokens depending on context. The term ```d'ye``` (used in the context of how d'ye do), is split into ```d``` and ```'ye```. Since ```d'``` is the part of the contraction that replaces the word do, I switched the apostrophe back over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gy0vOez6RReN"
   },
   "outputs": [],
   "source": [
    "#Don't lowercase initials (i.e. M.D.)\n",
    "import re\n",
    "r = re.compile('-[A-Z]{3}')\n",
    "remove_indices = []\n",
    "for i, doc in enumerate(text_pars):\n",
    "    for j, token in enumerate(doc):\n",
    "        if token.count('-') < 2 and token not in keep_upper_case and \\\n",
    "            token.find('.') <= 0 and len(token) > 2 and \\\n",
    "            token not in ['c', 'c.', 'd'] \\\n",
    "            and not(len(token)==1 and token.isupper()):\n",
    "            text_pars[i][j] = token.lower()\n",
    "        elif token.count('-') < 2 and (token.isupper() or token.islower()):\n",
    "            if token in ['A', 'a', 'I', 'I.'] or \\\n",
    "            (token[-1] == '.' and len(token) > 2):\n",
    "                if r.fullmatch(token):\n",
    "                    text_pars[i][j] = token + '-'\n",
    "                    text_pars[i][j+1] = '--'\n",
    "                else:\n",
    "                    text_pars[i][j] = token.lower()\n",
    "            elif 'c' in token:\n",
    "                remove_indices.append((i, j-1))\n",
    "                text_pars[i][j] = '&c.'\n",
    "                if text_pars[i][j+1] == '.':\n",
    "                    remove_indices.append((i, j+1))\n",
    "            elif token == 'd' or token == 'D':\n",
    "                if text_pars[i][j+1] == '-':\n",
    "                    text_pars[i][j] = 'd--m'\n",
    "                    remove_indices.append((i, j+1))\n",
    "                else:\n",
    "                    text_pars[i][j] = 'd\\''\n",
    "                    text_pars[i][j] = 'ye'\n",
    "            elif token == 'G' and text_pars[i][j-1].lower() == 'by':\n",
    "                text_pars[i][j] = 'God'\n",
    "                remove_indices.append((i, j+1))\n",
    "            elif '.' not in token and len(token) > 1:\n",
    "                text_pars[i][j] = token.lower()\n",
    "            elif token.isupper() and token[-1].isalpha() and token[0].isalpha():\n",
    "                text_pars[i][j] = token + '.'\n",
    "                remove_indices.append((i, j+1))\n",
    "for i, j in remove_indices:\n",
    "    text_pars[i] = text_pars[i][:j] + text_pars[i][j+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yE3XmbDQYlix"
   },
   "outputs": [],
   "source": [
    "max_words = 20000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words, filters='', lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Rrt8f9DYli6"
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KD_QMxx-YljF",
    "outputId": "68b42336-8ac5-4c60-e8d7-e55eb6c9cc74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13874"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hRxnz-HpYljM"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JhBRxSYGoh6J"
   },
   "outputs": [],
   "source": [
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5dVl7o9YljV"
   },
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "window_dist = window_size//2\n",
    "train_size = window_size - 1\n",
    "X_windows = []\n",
    "Y_labels = []\n",
    "\n",
    "# I tried using a sliding window with the y value pulled from the middle,\n",
    "## but couldn't work out how to implement text generation from the \n",
    "### finished model. Disappointing because models with data in this format\n",
    "#### saw accuracy jumps of 0.1\n",
    "# Sliding window to generate train data\n",
    "# for i in range(window_dist, len(seq)-window_dist):\n",
    "#     X_windows.append([*seq[i-window_dist:i], *seq[i+1:i+window_dist]])\n",
    "#     Y_labels.append(seq[i])\n",
    "\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(len(sequence)-window_size):\n",
    "        X_windows.append(sequence[i:i+train_size])\n",
    "        Y_labels.append(sequence[i+window_size])\n",
    "X = np.asarray(X_windows)\n",
    "Y = np.asarray(Y_labels).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dBBi2Z2olxM0",
    "outputId": "9f66ac53-6996-49eb-bbc3-8fc70b88943b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "853935"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to attempting the neural nets models, I had trained both a fasttext and a word2vec word embedding model on my data. I used these pretrained embeddings here.  \n",
    "*Note: I did not include these notebooks for submission because I ended up not using either of these embedding models. However, the notebooks in question are linked to in the readme of my submission folder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "La_pQwt99DFU",
    "outputId": "9899cfe8-22fb-4dbf-e1b2-6ec8f9d81815"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load('/content/drive/My Drive/Colab Notebooks/metis_proj4/models/fasttext_vectors.kv', mmap='r')\n",
    "# pretrained_embedding = word_vectors.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nWoKhnY6rNuS",
    "outputId": "72026747-21e0-494e-ac62-a37f2d8162d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13873"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reverse_word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ml9ucx-u8TTh"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_dim = word_vectors.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_vectors[word]\n",
    "    embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started out with two stacked LSTM layers following the Embedding layer. I chose this because I read a paper about the effectiveness of stacked LSTMs in text generation. However, after I abandoned the skipgram sliding window input data and my accuracy dropped, I added the Bidirectional layer to the first LSTM layer, which helped a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaPRA_wdxZHF"
   },
   "outputs": [],
   "source": [
    "model_ft = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=train_size, \n",
    "              weights=[embedding_matrix], trainable=False),\n",
    "    Bidirectional(LSTM(100, return_sequences=True)),\n",
    "    LSTM(200),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "A60ITRrhYljp",
    "outputId": "ada6c9ee-e8c8-437f-bdf0-11fee331d196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 19, 100)           1387400   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 19, 200)           160800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13874)             2094974   \n",
      "=================================================================\n",
      "Total params: 3,994,124\n",
      "Trainable params: 2,606,724\n",
      "Non-trainable params: 1,387,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These weights were included in my project submission, so this cell is runnable\n",
    "model_ft.load_weights('./models/model_weights_ft.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHkdxc1bYlj0"
   },
   "outputs": [],
   "source": [
    "model_ft.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "8Vrkrp8kYlj8",
    "outputId": "187c0c13-8e06-4da9-b5fe-e6db273843f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3336/3336 - 43s - loss: 4.7955 - accuracy: 0.1830\n",
      "Epoch 2/15\n",
      "3336/3336 - 43s - loss: 4.7346 - accuracy: 0.1852\n",
      "Epoch 3/15\n",
      "3336/3336 - 43s - loss: 4.6815 - accuracy: 0.1874\n",
      "Epoch 4/15\n",
      "3336/3336 - 42s - loss: 4.6335 - accuracy: 0.1890\n",
      "Epoch 5/15\n",
      "3336/3336 - 43s - loss: 4.5903 - accuracy: 0.1903\n",
      "Epoch 6/15\n",
      "3336/3336 - 43s - loss: 4.5500 - accuracy: 0.1919\n",
      "Epoch 7/15\n",
      "3336/3336 - 43s - loss: 4.5132 - accuracy: 0.1934\n",
      "Epoch 8/15\n",
      "3336/3336 - 42s - loss: 4.4801 - accuracy: 0.1948\n",
      "Epoch 9/15\n",
      "3336/3336 - 42s - loss: 4.4485 - accuracy: 0.1965\n",
      "Epoch 10/15\n",
      "3336/3336 - 43s - loss: 4.4187 - accuracy: 0.1982\n",
      "Epoch 11/15\n",
      "3336/3336 - 42s - loss: 4.3919 - accuracy: 0.2003\n",
      "Epoch 12/15\n",
      "3336/3336 - 43s - loss: 4.3659 - accuracy: 0.2019\n",
      "Epoch 13/15\n",
      "3336/3336 - 42s - loss: 4.3415 - accuracy: 0.2039\n",
      "Epoch 14/15\n",
      "3336/3336 - 43s - loss: 4.3186 - accuracy: 0.2057\n",
      "Epoch 15/15\n",
      "3336/3336 - 42s - loss: 4.2964 - accuracy: 0.2074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46e41c1e10>"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model_ft.fit(X, Y, batch_size=256, epochs=15, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YT90jBTEYlj_"
   },
   "outputs": [],
   "source": [
    "# model_ft.save('/content/drive/My Drive/models/model_weights_ft.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "UIauWYULwRnj",
    "outputId": "41d5f6fd-f3bd-40e8-cbd8-3af2a2e368ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "word_vectors_w2v = KeyedVectors.load('/content/drive/My Drive/Colab Notebooks/metis_proj4/models/word2vec_vectors.kv', mmap='r')\n",
    "pretrained_embedding = word_vectors_w2v.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1E2qHBFYlji"
   },
   "outputs": [],
   "source": [
    "model_w2v = Sequential([\n",
    "    pretrained_embedding,\n",
    "    Bidirectional(LSTM(100, return_sequences=True)),\n",
    "    LSTM(200, go_backwards=True),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These weights were also included in my project submission, so this cell is runnable\n",
    "model_w2v.load_weights('./models/model_weights_w2v.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "GQ0K-RAwwQdZ",
    "outputId": "f529555f-b364-4931-de72-81e2b1da8c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3336/3336 - 43s - loss: 4.6595 - accuracy: 0.1884\n",
      "Epoch 2/15\n",
      "3336/3336 - 43s - loss: 4.6249 - accuracy: 0.1893\n",
      "Epoch 3/15\n",
      "3336/3336 - 43s - loss: 4.6037 - accuracy: 0.1903\n",
      "Epoch 4/15\n",
      "3336/3336 - 43s - loss: 4.5828 - accuracy: 0.1910\n",
      "Epoch 5/15\n",
      "3336/3336 - 43s - loss: 4.5623 - accuracy: 0.1916\n",
      "Epoch 6/15\n",
      "3336/3336 - 43s - loss: 4.5420 - accuracy: 0.1921\n",
      "Epoch 7/15\n",
      "3336/3336 - 43s - loss: 4.5231 - accuracy: 0.1929\n",
      "Epoch 8/15\n",
      "3336/3336 - 43s - loss: 4.5034 - accuracy: 0.1937\n",
      "Epoch 9/15\n",
      "3336/3336 - 43s - loss: 4.4871 - accuracy: 0.1941\n",
      "Epoch 10/15\n",
      "3336/3336 - 43s - loss: 4.4659 - accuracy: 0.1945\n",
      "Epoch 11/15\n",
      "3336/3336 - 43s - loss: 4.4474 - accuracy: 0.1955\n",
      "Epoch 12/15\n",
      "3336/3336 - 43s - loss: 4.4303 - accuracy: 0.1959\n",
      "Epoch 13/15\n",
      "3336/3336 - 43s - loss: 4.4126 - accuracy: 0.1965\n",
      "Epoch 14/15\n",
      "3336/3336 - 43s - loss: 4.3955 - accuracy: 0.1973\n",
      "Epoch 15/15\n",
      "3336/3336 - 43s - loss: 4.3794 - accuracy: 0.1976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46dfbd9b70>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model_w2v.fit(X, Y, batch_size=256, epochs=15, verbose = 2)\n",
    "#Epoch 1: 0.1726, Epoch 15: 0.1867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-zqcRa23ZgZ"
   },
   "outputs": [],
   "source": [
    "model_w2v.save('/content/drive/My Drive/models/model_weights_w2v.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hCdyC84kIML"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=train_size),\n",
    "    Bidirectional(LSTM(100, return_sequences=True)),\n",
    "    LSTM(200),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not have the weights for this model as they were subsequently overwritten by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "bnS_USrekIMP",
    "outputId": "2b7f99a4-2340-4f9f-d6b7-4f477c0333a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1668/1668 - 47s - loss: 3.4903 - accuracy: 0.3040\n",
      "Epoch 2/15\n",
      "1668/1668 - 46s - loss: 3.4275 - accuracy: 0.3101\n",
      "Epoch 3/15\n",
      "1668/1668 - 46s - loss: 3.4021 - accuracy: 0.3138\n",
      "Epoch 4/15\n",
      "1668/1668 - 46s - loss: 3.3787 - accuracy: 0.3166\n",
      "Epoch 5/15\n",
      "1668/1668 - 46s - loss: 3.3571 - accuracy: 0.3192\n",
      "Epoch 6/15\n",
      "1668/1668 - 46s - loss: 3.3368 - accuracy: 0.3219\n",
      "Epoch 7/15\n",
      "1668/1668 - 46s - loss: 3.3174 - accuracy: 0.3246\n",
      "Epoch 8/15\n",
      "1668/1668 - 46s - loss: 3.2983 - accuracy: 0.3268\n",
      "Epoch 9/15\n",
      "1668/1668 - 46s - loss: 3.2828 - accuracy: 0.3291\n",
      "Epoch 10/15\n",
      "1668/1668 - 46s - loss: 3.2680 - accuracy: 0.3311\n",
      "Epoch 11/15\n",
      "1668/1668 - 46s - loss: 3.2484 - accuracy: 0.3330\n",
      "Epoch 12/15\n",
      "1668/1668 - 46s - loss: 3.2347 - accuracy: 0.3351\n",
      "Epoch 13/15\n",
      "1668/1668 - 46s - loss: 3.2198 - accuracy: 0.3374\n",
      "Epoch 14/15\n",
      "1668/1668 - 46s - loss: 3.2046 - accuracy: 0.3393\n",
      "Epoch 15/15\n",
      "1668/1668 - 46s - loss: 3.1919 - accuracy: 0.3413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46dd7205c0>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, Y, batch_size=512, epochs=15, verbose = 2)\n",
    "#Epoch 1: 0.2581, Epoch 15: 0.2963"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 15 epochs, the model performances with the three different embedding types (FastText, Word2Vec, and native Embedding), were:  \n",
    "FastTest - 42s - loss: 4.2964 - accuracy: 0.2074  \n",
    "Word2Vec - 43s - loss: 4.3794 - accuracy: 0.1976\n",
    "native Embedding layer - 46s - loss: 3.1919 - accuracy: 0.3413\n",
    "\n",
    "I played around with parameters a bit, but saw similar results each time, so I abandoned the pre-trained models and stuck to the native Embedding model instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The batch size of the native Embedding model is currently twice that of the previous two. However, I changed this after having compared the three models and finding the native Embedding one consistently superior with the same parameters as the other two. The main difference resulting from the change in batch size was that the runtime shortened.*  \n",
    "\n",
    "*Also, I've mentioned it a couple times so far, but there were a number of times when rather than copy and paste the model code, I just changed the code and reran the cell. Thus, I don't have access to all the model outputs, as I don't have time to rerun them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkcJA-C-kIMR"
   },
   "outputs": [],
   "source": [
    "# model.save('/content/drive/My Drive/models/model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to try implementing a Dropout layer, and wasn't sure whether to use the regular one or the Gaussian one, so I tried out both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3PRAx-qGaAD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, GaussianDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These weights were included in my project submission, so this cell is runnable\n",
    "model_2.load_weights('./models/model_2_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yh-WwsrmYlkE"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model_2 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=train_size),\n",
    "    Bidirectional(LSTM(100, return_sequences=True)),\n",
    "    LSTM(200),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dropout(rate=0.1),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "oosKDOtP_WwG",
    "outputId": "eca30424-3e25-493d-ddef-5cd6b91108c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1668/1668 - 47s - loss: 5.5922 - accuracy: 0.1359\n",
      "Epoch 2/15\n",
      "1668/1668 - 47s - loss: 5.2700 - accuracy: 0.1600\n",
      "Epoch 3/15\n",
      "1668/1668 - 46s - loss: 5.1357 - accuracy: 0.1695\n",
      "Epoch 4/15\n",
      "1668/1668 - 46s - loss: 5.0487 - accuracy: 0.1753\n",
      "Epoch 5/15\n",
      "1668/1668 - 46s - loss: 4.9732 - accuracy: 0.1799\n",
      "Epoch 6/15\n",
      "1668/1668 - 46s - loss: 4.9043 - accuracy: 0.1842\n",
      "Epoch 7/15\n",
      "1668/1668 - 46s - loss: 4.8407 - accuracy: 0.1882\n",
      "Epoch 8/15\n",
      "1668/1668 - 46s - loss: 4.7797 - accuracy: 0.1920\n",
      "Epoch 9/15\n",
      "1668/1668 - 46s - loss: 4.7228 - accuracy: 0.1957\n",
      "Epoch 10/15\n",
      "1668/1668 - 46s - loss: 4.6686 - accuracy: 0.1992\n",
      "Epoch 11/15\n",
      "1668/1668 - 46s - loss: 4.6160 - accuracy: 0.2021\n",
      "Epoch 12/15\n",
      "1668/1668 - 46s - loss: 4.5662 - accuracy: 0.2052\n",
      "Epoch 13/15\n",
      "1668/1668 - 46s - loss: 4.5173 - accuracy: 0.2083\n",
      "Epoch 14/15\n",
      "1668/1668 - 46s - loss: 4.4704 - accuracy: 0.2113\n",
      "Epoch 15/15\n",
      "1668/1668 - 46s - loss: 4.4255 - accuracy: 0.2142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46e6caefd0>"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model_2.fit(X, Y, batch_size=512, epochs=15, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't have the weights for this model. I'm not sure exactly why, but I think either I decided they weren't worth keeping or my runtime crashed and I had to restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q58Aj4P4GQ1I"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model_2_gauss = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=train_size),\n",
    "    Bidirectional(LSTM(100, return_sequences=True)),\n",
    "    LSTM(200),\n",
    "    Dense(150, activation='relu'),\n",
    "    GaussianDropout(rate=0.1),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "U8E5X5DIGQ1M",
    "outputId": "6c96d60a-5bb8-44ad-e873-1110a463e33d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1668/1668 - 46s - loss: 5.5860 - accuracy: 0.1368\n",
      "Epoch 2/15\n",
      "1668/1668 - 46s - loss: 5.2806 - accuracy: 0.1595\n",
      "Epoch 3/15\n",
      "1668/1668 - 46s - loss: 5.1405 - accuracy: 0.1684\n",
      "Epoch 4/15\n",
      "1668/1668 - 46s - loss: 5.0542 - accuracy: 0.1750\n",
      "Epoch 5/15\n",
      "1668/1668 - 46s - loss: 4.9824 - accuracy: 0.1796\n",
      "Epoch 6/15\n",
      "1668/1668 - 46s - loss: 4.9152 - accuracy: 0.1843\n",
      "Epoch 7/15\n",
      "1668/1668 - 46s - loss: 4.8530 - accuracy: 0.1881\n",
      "Epoch 8/15\n",
      "1668/1668 - 46s - loss: 4.7929 - accuracy: 0.1916\n",
      "Epoch 9/15\n",
      "1668/1668 - 46s - loss: 4.7361 - accuracy: 0.1948\n",
      "Epoch 10/15\n",
      "1668/1668 - 46s - loss: 4.6811 - accuracy: 0.1978\n",
      "Epoch 11/15\n",
      "1668/1668 - 46s - loss: 4.6287 - accuracy: 0.2011\n",
      "Epoch 12/15\n",
      "1668/1668 - 46s - loss: 4.5777 - accuracy: 0.2042\n",
      "Epoch 13/15\n",
      "1668/1668 - 46s - loss: 4.5295 - accuracy: 0.2071\n",
      "Epoch 14/15\n",
      "1668/1668 - 46s - loss: 4.4826 - accuracy: 0.2099\n",
      "Epoch 15/15\n",
      "1668/1668 - 46s - loss: 4.4390 - accuracy: 0.2131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46e42aa668>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_gauss.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model_2_gauss.fit(X, Y, batch_size=512, epochs=15, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0ODQXsDZ_j_"
   },
   "source": [
    "The Dropout layers didn't add to the aperformance so I dropped them.  \n",
    "The second LSTM layer has go_backwards=True because I was trying out having a forwards LSTM following by a backwards LSTM. That didn't help the performance much, but I accidentally forgot to change it back to forwards when I added the Bidirectional layer in. The performance jumped, and when I tried it out with the Bidirectional layer followed by a forwards LSTM layer, the performance went down again. So I stuck to the backwards layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euV9wQLOYlkV"
   },
   "outputs": [],
   "source": [
    "model_3 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=train_size),\n",
    "    Bidirectional(LSTM(100, return_sequences=True)),\n",
    "    LSTM(200, go_backwards=True),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These weights were included in my project submission, so this cell is runnable\n",
    "# This is the model that I accidentally overwrote model 1's weights with\n",
    "model_3.load_weights('./models/model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2g14s8QOFdZR"
   },
   "outputs": [],
   "source": [
    "# model_3.load_weights('/content/drive/My Drive/models/model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the below model may seem like a sudden jump, but this is actually after something like 400 epochs of training, where I kept running the model, saving the weights, and then loading them back up again and retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yBhC_upthBBP",
    "outputId": "53f27c2f-2e0c-404e-8010-39b2e2e750f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "834/834 - 51s - loss: 2.5574 - accuracy: 0.4364\n",
      "Epoch 2/200\n",
      "834/834 - 51s - loss: 2.4852 - accuracy: 0.4495\n",
      "Epoch 3/200\n",
      "834/834 - 51s - loss: 2.4706 - accuracy: 0.4526\n",
      "Epoch 4/200\n",
      "834/834 - 50s - loss: 2.4647 - accuracy: 0.4538\n",
      "Epoch 5/200\n",
      "834/834 - 50s - loss: 2.4665 - accuracy: 0.4536\n",
      "Epoch 6/200\n",
      "834/834 - 50s - loss: 2.4596 - accuracy: 0.4546\n",
      "Epoch 7/200\n",
      "834/834 - 50s - loss: 2.4817 - accuracy: 0.4503\n",
      "Epoch 8/200\n",
      "834/834 - 50s - loss: 2.4427 - accuracy: 0.4580\n",
      "Epoch 9/200\n",
      "834/834 - 50s - loss: 2.4390 - accuracy: 0.4582\n",
      "Epoch 10/200\n",
      "834/834 - 50s - loss: 2.4363 - accuracy: 0.4587\n",
      "Epoch 11/200\n",
      "834/834 - 50s - loss: 2.4335 - accuracy: 0.4596\n",
      "Epoch 12/200\n",
      "834/834 - 50s - loss: 2.4317 - accuracy: 0.4594\n",
      "Epoch 13/200\n",
      "834/834 - 50s - loss: 2.4291 - accuracy: 0.4600\n",
      "Epoch 14/200\n",
      "834/834 - 50s - loss: 2.4294 - accuracy: 0.4600\n",
      "Epoch 15/200\n",
      "834/834 - 51s - loss: 2.4262 - accuracy: 0.4608\n",
      "Epoch 16/200\n",
      "834/834 - 50s - loss: 2.4208 - accuracy: 0.4615\n",
      "Epoch 17/200\n",
      "834/834 - 50s - loss: 2.4178 - accuracy: 0.4620\n",
      "Epoch 18/200\n",
      "834/834 - 50s - loss: 2.4164 - accuracy: 0.4623\n",
      "Epoch 19/200\n",
      "834/834 - 50s - loss: 2.4118 - accuracy: 0.4634\n",
      "Epoch 20/200\n",
      "834/834 - 50s - loss: 2.4127 - accuracy: 0.4626\n",
      "Epoch 21/200\n",
      "834/834 - 50s - loss: 2.4116 - accuracy: 0.4627\n",
      "Epoch 22/200\n",
      "834/834 - 50s - loss: 2.4063 - accuracy: 0.4635\n",
      "Epoch 23/200\n",
      "834/834 - 50s - loss: 2.4036 - accuracy: 0.4643\n",
      "Epoch 24/200\n",
      "834/834 - 50s - loss: 2.4105 - accuracy: 0.4628\n",
      "Epoch 25/200\n",
      "834/834 - 50s - loss: 2.4046 - accuracy: 0.4642\n",
      "Epoch 26/200\n",
      "834/834 - 50s - loss: 2.3946 - accuracy: 0.4661\n",
      "Epoch 27/200\n",
      "834/834 - 50s - loss: 2.3917 - accuracy: 0.4664\n",
      "Epoch 28/200\n",
      "834/834 - 50s - loss: 2.3918 - accuracy: 0.4664\n",
      "Epoch 29/200\n",
      "834/834 - 50s - loss: 2.3947 - accuracy: 0.4659\n",
      "Epoch 30/200\n",
      "834/834 - 50s - loss: 2.3938 - accuracy: 0.4662\n",
      "Epoch 31/200\n",
      "834/834 - 50s - loss: 2.3919 - accuracy: 0.4664\n",
      "Epoch 32/200\n",
      "834/834 - 50s - loss: 2.3867 - accuracy: 0.4675\n",
      "Epoch 33/200\n",
      "834/834 - 50s - loss: 2.3854 - accuracy: 0.4674\n",
      "Epoch 34/200\n",
      "834/834 - 50s - loss: 2.3854 - accuracy: 0.4674\n",
      "Epoch 35/200\n",
      "834/834 - 50s - loss: 2.3826 - accuracy: 0.4679\n",
      "Epoch 36/200\n",
      "834/834 - 50s - loss: 2.3771 - accuracy: 0.4689\n",
      "Epoch 37/200\n",
      "834/834 - 50s - loss: 2.3767 - accuracy: 0.4690\n",
      "Epoch 38/200\n",
      "834/834 - 50s - loss: 2.3767 - accuracy: 0.4692\n",
      "Epoch 39/200\n",
      "834/834 - 50s - loss: 2.3725 - accuracy: 0.4697\n",
      "Epoch 40/200\n",
      "834/834 - 50s - loss: 2.3705 - accuracy: 0.4699\n",
      "Epoch 41/200\n",
      "834/834 - 50s - loss: 2.4966 - accuracy: 0.4503\n",
      "Epoch 42/200\n",
      "834/834 - 50s - loss: 2.4819 - accuracy: 0.4483\n",
      "Epoch 43/200\n",
      "834/834 - 50s - loss: 2.3495 - accuracy: 0.4738\n",
      "Epoch 44/200\n",
      "834/834 - 50s - loss: 2.3376 - accuracy: 0.4763\n",
      "Epoch 45/200\n",
      "834/834 - 50s - loss: 2.3522 - accuracy: 0.4733\n",
      "Epoch 46/200\n",
      "834/834 - 50s - loss: 2.3580 - accuracy: 0.4723\n",
      "Epoch 47/200\n",
      "834/834 - 50s - loss: 2.3609 - accuracy: 0.4717\n",
      "Epoch 48/200\n",
      "834/834 - 50s - loss: 2.3790 - accuracy: 0.4684\n",
      "Epoch 49/200\n",
      "834/834 - 50s - loss: 2.3646 - accuracy: 0.4707\n",
      "Epoch 50/200\n",
      "834/834 - 50s - loss: 2.3615 - accuracy: 0.4719\n",
      "Epoch 51/200\n",
      "834/834 - 50s - loss: 2.3536 - accuracy: 0.4730\n",
      "Epoch 52/200\n",
      "834/834 - 50s - loss: 2.3570 - accuracy: 0.4723\n",
      "Epoch 53/200\n",
      "834/834 - 50s - loss: 2.3576 - accuracy: 0.4721\n",
      "Epoch 54/200\n",
      "834/834 - 50s - loss: 2.3532 - accuracy: 0.4726\n",
      "Epoch 55/200\n",
      "834/834 - 50s - loss: 2.3539 - accuracy: 0.4725\n",
      "Epoch 56/200\n",
      "834/834 - 50s - loss: 2.3527 - accuracy: 0.4728\n",
      "Epoch 57/200\n",
      "834/834 - 50s - loss: 2.3542 - accuracy: 0.4729\n",
      "Epoch 58/200\n",
      "834/834 - 50s - loss: 2.3480 - accuracy: 0.4734\n",
      "Epoch 59/200\n",
      "834/834 - 50s - loss: 2.3425 - accuracy: 0.4748\n",
      "Epoch 60/200\n",
      "834/834 - 50s - loss: 2.3460 - accuracy: 0.4740\n",
      "Epoch 61/200\n",
      "834/834 - 50s - loss: 2.3459 - accuracy: 0.4743\n",
      "Epoch 62/200\n",
      "834/834 - 50s - loss: 2.3479 - accuracy: 0.4738\n",
      "Epoch 63/200\n",
      "834/834 - 50s - loss: 2.3385 - accuracy: 0.4754\n",
      "Epoch 64/200\n",
      "834/834 - 50s - loss: 2.3467 - accuracy: 0.4740\n",
      "Epoch 65/200\n",
      "834/834 - 50s - loss: 2.3452 - accuracy: 0.4740\n",
      "Epoch 66/200\n",
      "834/834 - 50s - loss: 2.3386 - accuracy: 0.4758\n",
      "Epoch 67/200\n",
      "834/834 - 50s - loss: 2.3392 - accuracy: 0.4752\n",
      "Epoch 68/200\n",
      "834/834 - 50s - loss: 2.3384 - accuracy: 0.4751\n",
      "Epoch 69/200\n",
      "834/834 - 50s - loss: 2.3401 - accuracy: 0.4748\n",
      "Epoch 70/200\n",
      "834/834 - 50s - loss: 2.3297 - accuracy: 0.4774\n",
      "Epoch 71/200\n",
      "834/834 - 50s - loss: 2.3294 - accuracy: 0.4772\n",
      "Epoch 72/200\n",
      "834/834 - 50s - loss: 2.3348 - accuracy: 0.4760\n",
      "Epoch 73/200\n",
      "834/834 - 50s - loss: 2.3309 - accuracy: 0.4764\n",
      "Epoch 74/200\n",
      "834/834 - 50s - loss: 2.3312 - accuracy: 0.4768\n",
      "Epoch 75/200\n",
      "834/834 - 50s - loss: 2.3313 - accuracy: 0.4767\n",
      "Epoch 76/200\n",
      "834/834 - 50s - loss: 2.3285 - accuracy: 0.4773\n",
      "Epoch 77/200\n",
      "834/834 - 50s - loss: 2.3241 - accuracy: 0.4778\n",
      "Epoch 78/200\n",
      "834/834 - 50s - loss: 2.3275 - accuracy: 0.4774\n",
      "Epoch 79/200\n",
      "834/834 - 50s - loss: 2.3231 - accuracy: 0.4781\n",
      "Epoch 80/200\n",
      "834/834 - 50s - loss: 2.3236 - accuracy: 0.4782\n",
      "Epoch 81/200\n",
      "834/834 - 50s - loss: 2.3247 - accuracy: 0.4778\n",
      "Epoch 82/200\n",
      "834/834 - 50s - loss: 2.3160 - accuracy: 0.4795\n",
      "Epoch 83/200\n",
      "834/834 - 50s - loss: 2.3223 - accuracy: 0.4782\n",
      "Epoch 84/200\n",
      "834/834 - 50s - loss: 2.3242 - accuracy: 0.4779\n",
      "Epoch 85/200\n",
      "834/834 - 50s - loss: 2.3184 - accuracy: 0.4790\n",
      "Epoch 86/200\n",
      "834/834 - 50s - loss: 2.3215 - accuracy: 0.4784\n",
      "Epoch 87/200\n",
      "834/834 - 50s - loss: 2.3197 - accuracy: 0.4786\n",
      "Epoch 88/200\n",
      "834/834 - 50s - loss: 2.3177 - accuracy: 0.4788\n",
      "Epoch 89/200\n",
      "834/834 - 50s - loss: 2.3145 - accuracy: 0.4792\n",
      "Epoch 90/200\n",
      "834/834 - 50s - loss: 2.3108 - accuracy: 0.4800\n",
      "Epoch 91/200\n",
      "834/834 - 50s - loss: 2.3128 - accuracy: 0.4799\n",
      "Epoch 92/200\n",
      "834/834 - 50s - loss: 2.3149 - accuracy: 0.4791\n",
      "Epoch 93/200\n",
      "834/834 - 50s - loss: 2.3098 - accuracy: 0.4797\n",
      "Epoch 94/200\n",
      "834/834 - 50s - loss: 2.3070 - accuracy: 0.4810\n",
      "Epoch 95/200\n",
      "834/834 - 50s - loss: 2.3088 - accuracy: 0.4800\n",
      "Epoch 96/200\n",
      "834/834 - 50s - loss: 2.3100 - accuracy: 0.4803\n",
      "Epoch 97/200\n",
      "834/834 - 50s - loss: 2.3087 - accuracy: 0.4804\n",
      "Epoch 98/200\n",
      "834/834 - 50s - loss: 2.3089 - accuracy: 0.4802\n",
      "Epoch 99/200\n",
      "834/834 - 50s - loss: 2.3037 - accuracy: 0.4816\n",
      "Epoch 100/200\n",
      "834/834 - 50s - loss: 2.3011 - accuracy: 0.4816\n",
      "Epoch 101/200\n",
      "834/834 - 50s - loss: 2.3045 - accuracy: 0.4807\n",
      "Epoch 102/200\n",
      "834/834 - 50s - loss: 2.3052 - accuracy: 0.4809\n",
      "Epoch 103/200\n",
      "834/834 - 50s - loss: 2.3035 - accuracy: 0.4814\n",
      "Epoch 104/200\n",
      "834/834 - 50s - loss: 2.3042 - accuracy: 0.4810\n",
      "Epoch 105/200\n",
      "834/834 - 50s - loss: 2.3011 - accuracy: 0.4817\n",
      "Epoch 106/200\n",
      "834/834 - 50s - loss: 2.2971 - accuracy: 0.4825\n",
      "Epoch 107/200\n",
      "834/834 - 50s - loss: 2.3124 - accuracy: 0.4794\n",
      "Epoch 108/200\n",
      "834/834 - 50s - loss: 2.3368 - accuracy: 0.4741\n",
      "Epoch 109/200\n",
      "834/834 - 50s - loss: 2.2824 - accuracy: 0.4853\n",
      "Epoch 110/200\n",
      "834/834 - 50s - loss: 2.2926 - accuracy: 0.4828\n",
      "Epoch 111/200\n",
      "834/834 - 50s - loss: 2.2990 - accuracy: 0.4822\n",
      "Epoch 112/200\n",
      "834/834 - 50s - loss: 2.2898 - accuracy: 0.4834\n",
      "Epoch 113/200\n",
      "834/834 - 50s - loss: 2.3001 - accuracy: 0.4821\n",
      "Epoch 114/200\n",
      "834/834 - 50s - loss: 2.2968 - accuracy: 0.4818\n",
      "Epoch 115/200\n",
      "834/834 - 50s - loss: 2.2928 - accuracy: 0.4830\n",
      "Epoch 116/200\n",
      "834/834 - 50s - loss: 2.2944 - accuracy: 0.4832\n",
      "Epoch 117/200\n",
      "834/834 - 50s - loss: 2.2922 - accuracy: 0.4827\n",
      "Epoch 118/200\n",
      "834/834 - 50s - loss: 2.2858 - accuracy: 0.4842\n",
      "Epoch 119/200\n",
      "834/834 - 50s - loss: 2.2974 - accuracy: 0.4817\n",
      "Epoch 120/200\n",
      "834/834 - 50s - loss: 2.2877 - accuracy: 0.4839\n",
      "Epoch 121/200\n",
      "834/834 - 50s - loss: 2.2801 - accuracy: 0.4851\n",
      "Epoch 122/200\n",
      "834/834 - 50s - loss: 2.2884 - accuracy: 0.4837\n",
      "Epoch 123/200\n",
      "834/834 - 50s - loss: 2.2894 - accuracy: 0.4835\n",
      "Epoch 124/200\n",
      "834/834 - 50s - loss: 2.2882 - accuracy: 0.4841\n",
      "Epoch 125/200\n",
      "834/834 - 50s - loss: 2.2874 - accuracy: 0.4835\n",
      "Epoch 126/200\n",
      "834/834 - 50s - loss: 2.2819 - accuracy: 0.4848\n",
      "Epoch 127/200\n",
      "834/834 - 50s - loss: 2.2884 - accuracy: 0.4837\n",
      "Epoch 128/200\n",
      "834/834 - 50s - loss: 2.2832 - accuracy: 0.4843\n",
      "Epoch 129/200\n",
      "834/834 - 50s - loss: 2.2814 - accuracy: 0.4847\n",
      "Epoch 130/200\n",
      "834/834 - 50s - loss: 2.2816 - accuracy: 0.4844\n",
      "Epoch 131/200\n",
      "834/834 - 50s - loss: 2.2772 - accuracy: 0.4855\n",
      "Epoch 132/200\n",
      "834/834 - 50s - loss: 2.2828 - accuracy: 0.4844\n",
      "Epoch 133/200\n",
      "834/834 - 50s - loss: 2.2800 - accuracy: 0.4853\n",
      "Epoch 134/200\n",
      "834/834 - 50s - loss: 2.2860 - accuracy: 0.4839\n",
      "Epoch 135/200\n",
      "834/834 - 50s - loss: 2.2853 - accuracy: 0.4840\n",
      "Epoch 136/200\n",
      "834/834 - 50s - loss: 2.2719 - accuracy: 0.4867\n",
      "Epoch 137/200\n",
      "834/834 - 50s - loss: 2.2760 - accuracy: 0.4857\n",
      "Epoch 138/200\n",
      "834/834 - 50s - loss: 2.2785 - accuracy: 0.4855\n",
      "Epoch 139/200\n",
      "834/834 - 50s - loss: 2.2700 - accuracy: 0.4869\n",
      "Epoch 140/200\n",
      "834/834 - 50s - loss: 2.2709 - accuracy: 0.4871\n",
      "Epoch 141/200\n",
      "834/834 - 50s - loss: 2.2841 - accuracy: 0.4841\n",
      "Epoch 142/200\n",
      "834/834 - 50s - loss: 2.2796 - accuracy: 0.4849\n",
      "Epoch 143/200\n",
      "834/834 - 50s - loss: 2.2761 - accuracy: 0.4856\n",
      "Epoch 144/200\n",
      "834/834 - 50s - loss: 2.2799 - accuracy: 0.4848\n",
      "Epoch 145/200\n",
      "834/834 - 50s - loss: 2.2705 - accuracy: 0.4864\n",
      "Epoch 146/200\n",
      "834/834 - 50s - loss: 2.2731 - accuracy: 0.4863\n",
      "Epoch 147/200\n",
      "834/834 - 50s - loss: 2.2710 - accuracy: 0.4865\n",
      "Epoch 148/200\n",
      "834/834 - 50s - loss: 2.2684 - accuracy: 0.4871\n",
      "Epoch 149/200\n",
      "834/834 - 50s - loss: 2.2731 - accuracy: 0.4861\n",
      "Epoch 150/200\n",
      "834/834 - 50s - loss: 2.2712 - accuracy: 0.4862\n",
      "Epoch 151/200\n",
      "834/834 - 50s - loss: 2.2677 - accuracy: 0.4869\n",
      "Epoch 152/200\n",
      "834/834 - 50s - loss: 2.2633 - accuracy: 0.4883\n",
      "Epoch 153/200\n",
      "834/834 - 50s - loss: 2.2637 - accuracy: 0.4874\n",
      "Epoch 154/200\n",
      "834/834 - 50s - loss: 2.2718 - accuracy: 0.4861\n",
      "Epoch 155/200\n",
      "834/834 - 50s - loss: 2.2687 - accuracy: 0.4871\n",
      "Epoch 156/200\n",
      "834/834 - 50s - loss: 2.2638 - accuracy: 0.4879\n",
      "Epoch 157/200\n",
      "834/834 - 50s - loss: 2.2638 - accuracy: 0.4878\n",
      "Epoch 158/200\n",
      "834/834 - 50s - loss: 2.2633 - accuracy: 0.4883\n",
      "Epoch 159/200\n",
      "834/834 - 50s - loss: 2.2623 - accuracy: 0.4881\n",
      "Epoch 160/200\n",
      "834/834 - 50s - loss: 2.2656 - accuracy: 0.4875\n",
      "Epoch 161/200\n",
      "834/834 - 50s - loss: 2.2634 - accuracy: 0.4878\n",
      "Epoch 162/200\n",
      "834/834 - 50s - loss: 2.2663 - accuracy: 0.4872\n",
      "Epoch 163/200\n",
      "834/834 - 50s - loss: 2.2556 - accuracy: 0.4893\n",
      "Epoch 164/200\n",
      "834/834 - 50s - loss: 2.2544 - accuracy: 0.4899\n",
      "Epoch 165/200\n",
      "834/834 - 50s - loss: 2.2624 - accuracy: 0.4878\n",
      "Epoch 166/200\n",
      "834/834 - 50s - loss: 2.2698 - accuracy: 0.4865\n",
      "Epoch 167/200\n",
      "834/834 - 50s - loss: 2.2598 - accuracy: 0.4881\n",
      "Epoch 168/200\n",
      "834/834 - 50s - loss: 2.2512 - accuracy: 0.4900\n",
      "Epoch 169/200\n",
      "834/834 - 50s - loss: 2.2605 - accuracy: 0.4883\n",
      "Epoch 170/200\n",
      "834/834 - 50s - loss: 2.2604 - accuracy: 0.4883\n",
      "Epoch 171/200\n",
      "834/834 - 50s - loss: 2.2539 - accuracy: 0.4893\n",
      "Epoch 172/200\n",
      "834/834 - 50s - loss: 2.2946 - accuracy: 0.4815\n",
      "Epoch 173/200\n",
      "834/834 - 50s - loss: 2.2528 - accuracy: 0.4899\n",
      "Epoch 174/200\n",
      "834/834 - 50s - loss: 2.2449 - accuracy: 0.4912\n",
      "Epoch 175/200\n",
      "834/834 - 50s - loss: 2.2481 - accuracy: 0.4906\n",
      "Epoch 176/200\n",
      "834/834 - 50s - loss: 2.2519 - accuracy: 0.4899\n",
      "Epoch 177/200\n",
      "834/834 - 50s - loss: 2.2547 - accuracy: 0.4895\n",
      "Epoch 178/200\n",
      "834/834 - 50s - loss: 2.2523 - accuracy: 0.4898\n",
      "Epoch 179/200\n",
      "834/834 - 50s - loss: 2.2527 - accuracy: 0.4897\n",
      "Epoch 180/200\n",
      "834/834 - 50s - loss: 2.2516 - accuracy: 0.4897\n",
      "Epoch 181/200\n",
      "834/834 - 50s - loss: 2.2527 - accuracy: 0.4893\n",
      "Epoch 182/200\n",
      "834/834 - 50s - loss: 2.2491 - accuracy: 0.4902\n",
      "Epoch 183/200\n",
      "834/834 - 50s - loss: 2.2480 - accuracy: 0.4906\n",
      "Epoch 184/200\n",
      "834/834 - 50s - loss: 2.2482 - accuracy: 0.4900\n",
      "Epoch 185/200\n",
      "834/834 - 50s - loss: 2.2406 - accuracy: 0.4918\n",
      "Epoch 186/200\n",
      "834/834 - 50s - loss: 2.2546 - accuracy: 0.4891\n",
      "Epoch 187/200\n",
      "834/834 - 50s - loss: 2.2506 - accuracy: 0.4901\n",
      "Epoch 188/200\n",
      "834/834 - 50s - loss: 2.2474 - accuracy: 0.4908\n",
      "Epoch 189/200\n",
      "834/834 - 50s - loss: 2.2517 - accuracy: 0.4900\n",
      "Epoch 190/200\n",
      "834/834 - 50s - loss: 2.2469 - accuracy: 0.4906\n",
      "Epoch 191/200\n",
      "834/834 - 50s - loss: 2.2452 - accuracy: 0.4907\n",
      "Epoch 192/200\n",
      "834/834 - 50s - loss: 2.2477 - accuracy: 0.4903\n",
      "Epoch 193/200\n",
      "834/834 - 50s - loss: 2.2471 - accuracy: 0.4901\n",
      "Epoch 194/200\n",
      "834/834 - 50s - loss: 2.2401 - accuracy: 0.4913\n",
      "Epoch 195/200\n",
      "834/834 - 50s - loss: 2.2421 - accuracy: 0.4916\n",
      "Epoch 196/200\n",
      "834/834 - 50s - loss: 2.2447 - accuracy: 0.4908\n",
      "Epoch 197/200\n",
      "834/834 - 50s - loss: 2.2496 - accuracy: 0.4900\n",
      "Epoch 198/200\n",
      "834/834 - 50s - loss: 2.2451 - accuracy: 0.4910\n",
      "Epoch 199/200\n",
      "834/834 - 50s - loss: 2.2451 - accuracy: 0.4913\n",
      "Epoch 200/200\n",
      "834/834 - 50s - loss: 2.2379 - accuracy: 0.4924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4688f41e48>"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model_3.fit(np.random.shuffle(X), Y, batch_size=1024, epochs=100, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qc2k3V7vWehy"
   },
   "outputs": [],
   "source": [
    "model_3.save('/content/drive/My Drive/models/model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmUihOD9bGD8"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QRveAZlCbUGe"
   },
   "outputs": [],
   "source": [
    "model_4 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=train_size),\n",
    "    Bidirectional(LSTM(100, return_sequences=True)),\n",
    "    LSTM(200, go_backwards=True),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9LWnEv3bUGm"
   },
   "outputs": [],
   "source": [
    "# model_4.load_weights('/content/drive/My Drive/models/model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filepath in the below cell in wrong :( so I don't have the weights for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HChHTXwVbq9K",
    "outputId": "93e75afb-d025-4756-928e-83e5690deb27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.5586 - accuracy: 0.4357\n",
      "Epoch 00001: loss improved from inf to 2.55860, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.5586 - accuracy: 0.4357\n",
      "Epoch 2/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4868 - accuracy: 0.4499\n",
      "Epoch 00002: loss improved from 2.55860 to 2.48676, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.4868 - accuracy: 0.4499\n",
      "Epoch 3/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4714 - accuracy: 0.4522\n",
      "Epoch 00003: loss improved from 2.48676 to 2.47141, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.4714 - accuracy: 0.4522\n",
      "Epoch 4/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4696 - accuracy: 0.4527\n",
      "Epoch 00004: loss improved from 2.47141 to 2.46960, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.4696 - accuracy: 0.4527\n",
      "Epoch 5/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4623 - accuracy: 0.4540\n",
      "Epoch 00005: loss improved from 2.46960 to 2.46227, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.4623 - accuracy: 0.4540\n",
      "Epoch 6/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4567 - accuracy: 0.4554\n",
      "Epoch 00006: loss improved from 2.46227 to 2.45665, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.4567 - accuracy: 0.4554\n",
      "Epoch 7/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4539 - accuracy: 0.4555\n",
      "Epoch 00007: loss improved from 2.45665 to 2.45393, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.4539 - accuracy: 0.4555\n",
      "Epoch 8/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4495 - accuracy: 0.4564\n",
      "Epoch 00008: loss improved from 2.45393 to 2.44953, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4495 - accuracy: 0.4564\n",
      "Epoch 9/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4434 - accuracy: 0.4575\n",
      "Epoch 00009: loss improved from 2.44953 to 2.44339, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.4434 - accuracy: 0.4575\n",
      "Epoch 10/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4392 - accuracy: 0.4584\n",
      "Epoch 00010: loss improved from 2.44339 to 2.43923, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4392 - accuracy: 0.4584\n",
      "Epoch 11/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4368 - accuracy: 0.4590\n",
      "Epoch 00011: loss improved from 2.43923 to 2.43676, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4368 - accuracy: 0.4590\n",
      "Epoch 12/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4314 - accuracy: 0.4596\n",
      "Epoch 00012: loss improved from 2.43676 to 2.43139, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4314 - accuracy: 0.4596\n",
      "Epoch 13/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4275 - accuracy: 0.4605\n",
      "Epoch 00013: loss improved from 2.43139 to 2.42745, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4275 - accuracy: 0.4605\n",
      "Epoch 14/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4274 - accuracy: 0.4601\n",
      "Epoch 00014: loss improved from 2.42745 to 2.42739, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4274 - accuracy: 0.4601\n",
      "Epoch 15/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4234 - accuracy: 0.4617\n",
      "Epoch 00015: loss improved from 2.42739 to 2.42341, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.4234 - accuracy: 0.4617\n",
      "Epoch 16/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4230 - accuracy: 0.4609\n",
      "Epoch 00016: loss improved from 2.42341 to 2.42296, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4230 - accuracy: 0.4609\n",
      "Epoch 17/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4171 - accuracy: 0.4622\n",
      "Epoch 00017: loss improved from 2.42296 to 2.41714, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4171 - accuracy: 0.4622\n",
      "Epoch 18/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4123 - accuracy: 0.4633\n",
      "Epoch 00018: loss improved from 2.41714 to 2.41234, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4123 - accuracy: 0.4633\n",
      "Epoch 19/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4145 - accuracy: 0.4630\n",
      "Epoch 00019: loss did not improve from 2.41234\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4145 - accuracy: 0.4630\n",
      "Epoch 20/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4139 - accuracy: 0.4628\n",
      "Epoch 00020: loss did not improve from 2.41234\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4139 - accuracy: 0.4628\n",
      "Epoch 21/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4056 - accuracy: 0.4645\n",
      "Epoch 00021: loss improved from 2.41234 to 2.40555, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4056 - accuracy: 0.4645\n",
      "Epoch 22/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4034 - accuracy: 0.4650\n",
      "Epoch 00022: loss improved from 2.40555 to 2.40336, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4034 - accuracy: 0.4650\n",
      "Epoch 23/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4041 - accuracy: 0.4645\n",
      "Epoch 00023: loss did not improve from 2.40336\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4041 - accuracy: 0.4645\n",
      "Epoch 24/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4010 - accuracy: 0.4651\n",
      "Epoch 00024: loss improved from 2.40336 to 2.40098, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4010 - accuracy: 0.4651\n",
      "Epoch 25/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.4046 - accuracy: 0.4640\n",
      "Epoch 00025: loss did not improve from 2.40098\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.4046 - accuracy: 0.4640\n",
      "Epoch 26/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3993 - accuracy: 0.4653\n",
      "Epoch 00026: loss improved from 2.40098 to 2.39931, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3993 - accuracy: 0.4653\n",
      "Epoch 27/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3941 - accuracy: 0.4661\n",
      "Epoch 00027: loss improved from 2.39931 to 2.39413, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3941 - accuracy: 0.4661\n",
      "Epoch 28/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3927 - accuracy: 0.4663\n",
      "Epoch 00028: loss improved from 2.39413 to 2.39272, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3927 - accuracy: 0.4663\n",
      "Epoch 29/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3925 - accuracy: 0.4664\n",
      "Epoch 00029: loss improved from 2.39272 to 2.39248, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3925 - accuracy: 0.4664\n",
      "Epoch 30/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3888 - accuracy: 0.4674\n",
      "Epoch 00030: loss improved from 2.39248 to 2.38882, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3888 - accuracy: 0.4674\n",
      "Epoch 31/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3861 - accuracy: 0.4678\n",
      "Epoch 00031: loss improved from 2.38882 to 2.38614, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3861 - accuracy: 0.4678\n",
      "Epoch 32/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3849 - accuracy: 0.4676\n",
      "Epoch 00032: loss improved from 2.38614 to 2.38492, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3849 - accuracy: 0.4676\n",
      "Epoch 33/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3809 - accuracy: 0.4685\n",
      "Epoch 00033: loss improved from 2.38492 to 2.38091, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 53s 63ms/step - loss: 2.3809 - accuracy: 0.4685\n",
      "Epoch 34/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3839 - accuracy: 0.4681\n",
      "Epoch 00034: loss did not improve from 2.38091\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3839 - accuracy: 0.4681\n",
      "Epoch 35/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3819 - accuracy: 0.4685\n",
      "Epoch 00035: loss did not improve from 2.38091\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3819 - accuracy: 0.4685\n",
      "Epoch 36/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3787 - accuracy: 0.4687\n",
      "Epoch 00036: loss improved from 2.38091 to 2.37875, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3787 - accuracy: 0.4687\n",
      "Epoch 37/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3794 - accuracy: 0.4688\n",
      "Epoch 00037: loss did not improve from 2.37875\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3794 - accuracy: 0.4688\n",
      "Epoch 38/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3761 - accuracy: 0.4690\n",
      "Epoch 00038: loss improved from 2.37875 to 2.37609, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3761 - accuracy: 0.4690\n",
      "Epoch 39/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3742 - accuracy: 0.4698\n",
      "Epoch 00039: loss improved from 2.37609 to 2.37425, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3742 - accuracy: 0.4698\n",
      "Epoch 40/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3759 - accuracy: 0.4692\n",
      "Epoch 00040: loss did not improve from 2.37425\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3759 - accuracy: 0.4692\n",
      "Epoch 41/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3708 - accuracy: 0.4701\n",
      "Epoch 00041: loss improved from 2.37425 to 2.37083, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3708 - accuracy: 0.4701\n",
      "Epoch 42/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3659 - accuracy: 0.4713\n",
      "Epoch 00042: loss improved from 2.37083 to 2.36589, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3659 - accuracy: 0.4713\n",
      "Epoch 43/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3690 - accuracy: 0.4706\n",
      "Epoch 00043: loss did not improve from 2.36589\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3690 - accuracy: 0.4706\n",
      "Epoch 44/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3820 - accuracy: 0.4679\n",
      "Epoch 00044: loss did not improve from 2.36589\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3820 - accuracy: 0.4679\n",
      "Epoch 45/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3694 - accuracy: 0.4701\n",
      "Epoch 00045: loss did not improve from 2.36589\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3694 - accuracy: 0.4701\n",
      "Epoch 46/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3564 - accuracy: 0.4724\n",
      "Epoch 00046: loss improved from 2.36589 to 2.35645, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3564 - accuracy: 0.4724\n",
      "Epoch 47/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3574 - accuracy: 0.4727\n",
      "Epoch 00047: loss did not improve from 2.35645\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3574 - accuracy: 0.4727\n",
      "Epoch 48/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3635 - accuracy: 0.4708\n",
      "Epoch 00048: loss did not improve from 2.35645\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3635 - accuracy: 0.4708\n",
      "Epoch 49/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3595 - accuracy: 0.4722\n",
      "Epoch 00049: loss did not improve from 2.35645\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3595 - accuracy: 0.4722\n",
      "Epoch 50/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3576 - accuracy: 0.4721\n",
      "Epoch 00050: loss did not improve from 2.35645\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3576 - accuracy: 0.4721\n",
      "Epoch 51/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3598 - accuracy: 0.4718\n",
      "Epoch 00051: loss did not improve from 2.35645\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3598 - accuracy: 0.4718\n",
      "Epoch 52/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3596 - accuracy: 0.4720\n",
      "Epoch 00052: loss did not improve from 2.35645\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3596 - accuracy: 0.4720\n",
      "Epoch 53/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3573 - accuracy: 0.4728\n",
      "Epoch 00053: loss did not improve from 2.35645\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3573 - accuracy: 0.4728\n",
      "Epoch 54/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3482 - accuracy: 0.4740\n",
      "Epoch 00054: loss improved from 2.35645 to 2.34815, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3482 - accuracy: 0.4740\n",
      "Epoch 55/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3495 - accuracy: 0.4736\n",
      "Epoch 00055: loss did not improve from 2.34815\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3495 - accuracy: 0.4736\n",
      "Epoch 56/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3515 - accuracy: 0.4729\n",
      "Epoch 00056: loss did not improve from 2.34815\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3515 - accuracy: 0.4729\n",
      "Epoch 57/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3500 - accuracy: 0.4732\n",
      "Epoch 00057: loss did not improve from 2.34815\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3500 - accuracy: 0.4732\n",
      "Epoch 58/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3439 - accuracy: 0.4746\n",
      "Epoch 00058: loss improved from 2.34815 to 2.34389, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3439 - accuracy: 0.4746\n",
      "Epoch 59/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3434 - accuracy: 0.4748\n",
      "Epoch 00059: loss improved from 2.34389 to 2.34342, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3434 - accuracy: 0.4748\n",
      "Epoch 60/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3449 - accuracy: 0.4745\n",
      "Epoch 00060: loss did not improve from 2.34342\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3449 - accuracy: 0.4745\n",
      "Epoch 61/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3446 - accuracy: 0.4746\n",
      "Epoch 00061: loss did not improve from 2.34342\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3446 - accuracy: 0.4746\n",
      "Epoch 62/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3472 - accuracy: 0.4740\n",
      "Epoch 00062: loss did not improve from 2.34342\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3472 - accuracy: 0.4740\n",
      "Epoch 63/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3506 - accuracy: 0.4734\n",
      "Epoch 00063: loss did not improve from 2.34342\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3506 - accuracy: 0.4734\n",
      "Epoch 64/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3356 - accuracy: 0.4757\n",
      "Epoch 00064: loss improved from 2.34342 to 2.33565, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3356 - accuracy: 0.4757\n",
      "Epoch 65/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3360 - accuracy: 0.4763\n",
      "Epoch 00065: loss did not improve from 2.33565\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3360 - accuracy: 0.4763\n",
      "Epoch 66/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3322 - accuracy: 0.4765\n",
      "Epoch 00066: loss improved from 2.33565 to 2.33215, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3322 - accuracy: 0.4765\n",
      "Epoch 67/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3377 - accuracy: 0.4752\n",
      "Epoch 00067: loss did not improve from 2.33215\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3377 - accuracy: 0.4752\n",
      "Epoch 68/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3331 - accuracy: 0.4764\n",
      "Epoch 00068: loss did not improve from 2.33215\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3331 - accuracy: 0.4764\n",
      "Epoch 69/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3336 - accuracy: 0.4766\n",
      "Epoch 00069: loss did not improve from 2.33215\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3336 - accuracy: 0.4766\n",
      "Epoch 70/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3352 - accuracy: 0.4758\n",
      "Epoch 00070: loss did not improve from 2.33215\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3352 - accuracy: 0.4758\n",
      "Epoch 71/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3336 - accuracy: 0.4762\n",
      "Epoch 00071: loss did not improve from 2.33215\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3336 - accuracy: 0.4762\n",
      "Epoch 72/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3306 - accuracy: 0.4767\n",
      "Epoch 00072: loss improved from 2.33215 to 2.33061, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3306 - accuracy: 0.4767\n",
      "Epoch 73/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3313 - accuracy: 0.4770\n",
      "Epoch 00073: loss did not improve from 2.33061\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3313 - accuracy: 0.4770\n",
      "Epoch 74/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3273 - accuracy: 0.4773\n",
      "Epoch 00074: loss improved from 2.33061 to 2.32728, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3273 - accuracy: 0.4773\n",
      "Epoch 75/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3256 - accuracy: 0.4777\n",
      "Epoch 00075: loss improved from 2.32728 to 2.32560, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3256 - accuracy: 0.4777\n",
      "Epoch 76/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3270 - accuracy: 0.4774\n",
      "Epoch 00076: loss did not improve from 2.32560\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3270 - accuracy: 0.4774\n",
      "Epoch 77/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3248 - accuracy: 0.4776\n",
      "Epoch 00077: loss improved from 2.32560 to 2.32476, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3248 - accuracy: 0.4776\n",
      "Epoch 78/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3347 - accuracy: 0.4759\n",
      "Epoch 00078: loss did not improve from 2.32476\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3347 - accuracy: 0.4759\n",
      "Epoch 79/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3272 - accuracy: 0.4772\n",
      "Epoch 00079: loss did not improve from 2.32476\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3272 - accuracy: 0.4772\n",
      "Epoch 80/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3214 - accuracy: 0.4782\n",
      "Epoch 00080: loss improved from 2.32476 to 2.32136, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3214 - accuracy: 0.4782\n",
      "Epoch 81/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3171 - accuracy: 0.4790\n",
      "Epoch 00081: loss improved from 2.32136 to 2.31708, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3171 - accuracy: 0.4790\n",
      "Epoch 82/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3208 - accuracy: 0.4784\n",
      "Epoch 00082: loss did not improve from 2.31708\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3208 - accuracy: 0.4784\n",
      "Epoch 83/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3198 - accuracy: 0.4785\n",
      "Epoch 00083: loss did not improve from 2.31708\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3198 - accuracy: 0.4785\n",
      "Epoch 84/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3191 - accuracy: 0.4784\n",
      "Epoch 00084: loss did not improve from 2.31708\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3191 - accuracy: 0.4784\n",
      "Epoch 85/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3207 - accuracy: 0.4782\n",
      "Epoch 00085: loss did not improve from 2.31708\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3207 - accuracy: 0.4782\n",
      "Epoch 86/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3192 - accuracy: 0.4785\n",
      "Epoch 00086: loss did not improve from 2.31708\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3192 - accuracy: 0.4785\n",
      "Epoch 87/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3103 - accuracy: 0.4799\n",
      "Epoch 00087: loss improved from 2.31708 to 2.31030, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3103 - accuracy: 0.4799\n",
      "Epoch 88/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3128 - accuracy: 0.4797\n",
      "Epoch 00088: loss did not improve from 2.31030\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3128 - accuracy: 0.4797\n",
      "Epoch 89/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3153 - accuracy: 0.4795\n",
      "Epoch 00089: loss did not improve from 2.31030\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3153 - accuracy: 0.4795\n",
      "Epoch 90/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3176 - accuracy: 0.4788\n",
      "Epoch 00090: loss did not improve from 2.31030\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3176 - accuracy: 0.4788\n",
      "Epoch 91/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3216 - accuracy: 0.4776\n",
      "Epoch 00091: loss did not improve from 2.31030\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3216 - accuracy: 0.4776\n",
      "Epoch 92/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3117 - accuracy: 0.4800\n",
      "Epoch 00092: loss did not improve from 2.31030\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3117 - accuracy: 0.4800\n",
      "Epoch 93/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3066 - accuracy: 0.4807\n",
      "Epoch 00093: loss improved from 2.31030 to 2.30657, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3066 - accuracy: 0.4807\n",
      "Epoch 94/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3100 - accuracy: 0.4803\n",
      "Epoch 00094: loss did not improve from 2.30657\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3100 - accuracy: 0.4803\n",
      "Epoch 95/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3059 - accuracy: 0.4808\n",
      "Epoch 00095: loss improved from 2.30657 to 2.30586, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 63ms/step - loss: 2.3059 - accuracy: 0.4808\n",
      "Epoch 96/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3100 - accuracy: 0.4801\n",
      "Epoch 00096: loss did not improve from 2.30586\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3100 - accuracy: 0.4801\n",
      "Epoch 97/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3022 - accuracy: 0.4814\n",
      "Epoch 00097: loss improved from 2.30586 to 2.30224, saving model to ./model_4_weights_sg.hdf5\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3022 - accuracy: 0.4814\n",
      "Epoch 98/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3048 - accuracy: 0.4810\n",
      "Epoch 00098: loss did not improve from 2.30224\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3048 - accuracy: 0.4810\n",
      "Epoch 99/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3046 - accuracy: 0.4807\n",
      "Epoch 00099: loss did not improve from 2.30224\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3046 - accuracy: 0.4807\n",
      "Epoch 100/100\n",
      "834/834 [==============================] - ETA: 0s - loss: 2.3050 - accuracy: 0.4814\n",
      "Epoch 00100: loss did not improve from 2.30224\n",
      "834/834 [==============================] - 52s 62ms/step - loss: 2.3050 - accuracy: 0.4814\n"
     ]
    }
   ],
   "source": [
    "# # Early stopping allows model to stop training if improvement stops.\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "model_4.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "filepath = \"./model_4_weights_sg.hdf5\"\n",
    "# # Model checkpointing allows us to preserve progress during training if training is interrupted\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "history_4 = model_4.fit(X, Y, epochs = 100, batch_size = 1024, callbacks = callbacks_list, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAVXuQZvYlki"
   },
   "outputs": [],
   "source": [
    "def gen(model, input_str, max_len = 20):\n",
    "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
    "    reaches max_len'''\n",
    "    # Tokenize the input string\n",
    "    tokenized_sent = tokenizer.texts_to_sequences([word_tokenize(input_str)])[0]\n",
    "    while len(tokenized_sent) < max_len:\n",
    "        padded_sentence = pad_sequences([tokenized_sent[-19:]], maxlen=19)[0]\n",
    "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
    "        tokenized_sent.append(op.argmax()+1)\n",
    "        \n",
    "    return \" \".join(map(lambda x : reverse_word_map[x], tokenized_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-x7xjakYlkp"
   },
   "outputs": [],
   "source": [
    "model_list = [model, model_2, model_3, model_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynsWKFHMYlkm"
   },
   "outputs": [],
   "source": [
    "def test_models(test_string, sequence_length= 50, model_list = model_list):\n",
    "    '''Generates output given input test_string up to sequence_length'''\n",
    "    print('Input String: ', test_string)\n",
    "    for counter,model in enumerate(model_list):\n",
    "        print(\"Model \", counter+1, \":\")\n",
    "        print(gen(model,test_string,sequence_length))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure what't up with the outputs of these cells. For some reason the output of models 2 and 3 are completely sparsified, which is very strange, since they were not before I downloaded this notebook off Google Colab. For reference, model 2's output was similar to model 1's, but with more repeated words, while model 3's output was similar to model 4's in terms of comprehensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "VgdZUBMmYlkx",
    "outputId": "0c51c017-50f0-47c8-ac11-ffebd1c000dd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP-\n",
      "Model  1 :\n",
      "-BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- to to wherever not . in a were moral she . not rapid join recollected respectability with of the the the the the the -LNM- of not . in contrasted to wherever in candlelight on calm a . season do more when youthful she -PLC- . , estimation restless . fourth - , ground she . , sees and whenever features her . reasonably to benches cheese which as as as as on dined to poor not as her in woman as . in disputable rest in a . in gentleness with honour were be be was voices be and and intercourse might . in . in so consequence ; in a and leaving having her she and . to to not be . in . not to violated you her -LNM- before the . fortitude eighteen engaging prison is plays ecstasy the the the the the the the the the the the were he what sailors confused to . , her of distinct . as in gilding than to think . work -FNM- to to pianoforte of . not to billiard-table noticed not be . no a a a\n",
      "Model  2 :\n",
      "-BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- to to . in a a a . in a a . in . in . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , . , . , . , . , . , . , been . , . , .\n",
      "Model  3 :\n",
      "-BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "Model  4 :\n",
      "-BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- to begin handsome . in ; i thrown to to talks or . at ceased art . -PLC- behind party dear is him . . . is continually increase story to inattention dear not of the the the the the the the the the the the the the the -LNM- after the . not to shrubbery significantly less to . employ a a a were i and -LNM- of . in great her next . long to appearing and be . '' , tenderness to walked winter of while side her importance i would i -FNM- . , breeding can her concern of tried steps which dearer could to the the what an praise -FNM- enough and the a a grooms date and a . to have hair . . notice i equal she had appeared the . at . . `` chat to not it it it it what remain it what let striking agreed or what earnest -FNM- seemed gives checked guest miserable more when in has is again deferred can him husband , own whose will called . said not not other . '' was project but\n"
     ]
    }
   ],
   "source": [
    "test_models('-BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP- -BGP-', 200, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "B9Oni1UXYlk2",
    "outputId": "d9a76e26-35d1-4182-85e6-c623817796f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  It is a truth universally acknowledged\n",
      "Model  1 :\n",
      "It is a truth universally acknowledged was and gracious the a a a a a a a a . to -MNM- . is so . '' was complaints suppose is so and slightest first it was was was it it it it it it it it it it it be own - . in almost house have her fashion fairly but and could message the a the a a loss , rather ! , considerably to to not to . was and . in us was and . . . . at . at little bought having her and not believe a a a a a looking a . in all hesitation then -LNM- qualified and and guarded none be brothers marriage who i . in . at . is leave . at little only her to her completion . , had would her and and and and what as in riotous and that -FNM- is -FNM- and travelling . doted to beg through companion lines - . at ceased the a and head everybody -FNM- and . , . in ; convinced have her and As not the . at . . maintain he . is . at ceased\n",
      "Model  2 :\n",
      "It is a truth universally acknowledged it it -FNM- -FNM- -FNM- -FNM- -FNM- -FNM- -FNM- and the a a a the a the the the the the the the the the the the the the the . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . ,\n",
      "Model  3 :\n",
      "It is a truth universally acknowledged , , , , , , , , , , , , , , , , , , knowing , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "Model  4 :\n",
      "It is a truth universally acknowledged of whole , day winter . in the young in places chaise sight to not you . existence house which is decorum it it it it it it it it it it it it it number , time it -FNM- common spite help . '' , kept parents self-possession let afterwards i `` resolve -MNM- you and all interesting and and but and what We and the all of these only the the the were she and . dread kind -MNM- ensued sort sorrow scheme termination no . not speech you the . to however adequate the the the the the . any will which only a cousins only . . promised intimacy could fatigues of - with consequence i what apples and . three and at comforts four carriages on observations quite spreading . you bound and . in to side said . at ceased right - awakened . things experienced . had would i domestic and natural be does of what forty at before had entirely the the the what given are the a a two the a . proof not spite for spirits to motive to a demanded ways to and\n"
     ]
    }
   ],
   "source": [
    "test_models('It is a truth universally acknowledged', 200, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "0Omotty7Ylk5",
    "outputId": "882c6cb7-d8c3-44f2-f45a-d8a79fd07dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  You must allow me to tell you\n",
      "Model  1 :\n",
      "must allow me to tell you to designs saying a a being her and lively the the it it was it it it it it , was and connections mentioning morning her nurse of -LNM- who `` , always . '' was unexpected and and . duties and . '' was to and and be and . all to be and . in her homewards interrupted but . her hopeless they and thirteen . , paper her in bye be how was . wearing had attentive sister-in-law the . ; in a almost . while very for poetical recommendation must deserve to stood ill his a . to a a . always to to to to arms was and correct a because they and although hills be . as in not General all relating a in grove idea with not her -FNM- the the modern the what is the and complying , was and forbid . , . in ; in consequence to a in mourning must sisters copied of claimed not the ready warmly of of . not to to however and ill-timed be . prospects i `` civil he had and or could that . ; was her\n",
      "Model  2 :\n",
      "must allow me to tell you was and and and the a a the what had . , . , . , . , . , . , . , . , . , been and . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . , . , . , . , . , . , . , been . , been . ,\n",
      "Model  3 :\n",
      "must allow me to tell you , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "Model  4 :\n",
      "must allow me to tell you convinced skill to her balls - . , an . . . . had indeed . glad . warmest countenance -MNM- home in came like them as advantageously . in in great . in a him becomes fire noise -LNM- - too when picture occurred had display small of the the the the the a a for ; . proved -FNM- requesting spared ; in small walked . to untouched certainly filled side to . a a a that them which i and sensibility -FNM- least liked domestic the . - wholly . river -FNM- heinous two -LNM- it it it it -LNM- . , bowed it he and only what an a what they the a the the the . , should suppose in certainly certainly into but to route to the the -LNM- it he , was twelve well this young and not reflected and . not garden this and ease think good-will first - careless . you to of . '' , through infinite of of of of . '' was valuable charge very his cheerfully to the -LNM- to . was hers sight house looks consulted has hope friends his\n"
     ]
    }
   ],
   "source": [
    "test_models('You must allow me to tell you', 200, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "au3F_oj_D8S3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "neural_nets_sg_masking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
